{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Deduplication Workflow\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load config from `config.yml`.\n",
    "2. Load and unify Dataset1 + Dataset2 (in Dataset1â€™s format).\n",
    "3. Block and fuzzy-match records.\n",
    "4. Merge duplicates.\n",
    "5. Save final combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.fuzzy_helpers import (\n",
    "    load_config,\n",
    "    load_dataset1,\n",
    "    load_dataset2_and_map,\n",
    "    combine_dataframes,\n",
    "    block_data,\n",
    "    build_comparison_features,\n",
    "    classify_duplicates,\n",
    "    merge_duplicate_pairs,\n",
    "    keep_dataset1_and_unmatched_dataset2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Configuration\n",
    "\n",
    "Our `config.yml` file contains:\n",
    "- dataset1_path\n",
    "- dataset2_path\n",
    "- output_path\n",
    "- dataset1_columns\n",
    "- dataset2_to_dataset1\n",
    "- blocking_keys\n",
    "- fuzzy_keys\n",
    "- similarity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.yml\"  # Adjust if needed\n",
    "config = load_config(config_path)\n",
    "\n",
    "dataset1_path = config[\"dataset1_path\"]\n",
    "dataset2_path = config[\"dataset2_path\"]\n",
    "output_path = config[\"output_path\"]\n",
    "\n",
    "dataset1_columns = config[\"dataset1_columns\"]\n",
    "dataset2_to_dataset1 = config[\"dataset2_to_dataset1\"]\n",
    "blocking_keys = config[\"blocking_keys\"]\n",
    "fuzzy_keys = config[\"fuzzy_keys\"]\n",
    "similarity_threshold = float(config[\"similarity_threshold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load & Clean Datasets\n",
    "\n",
    "Dataset1 is presumably already in the final schema, but we ensure all columns exist.  \n",
    "Dataset2 is mapped to that same schema using the provided dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_dataset1(dataset1_path, dataset1_columns)\n",
    "df2_mapped = load_dataset2_and_map(dataset2_path, dataset1_columns, dataset2_to_dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing step #1: Convert these date columns to \"MM/DD/YYYY\"\n",
    "date_cols = []\n",
    "\n",
    "for col in date_cols:\n",
    "    if col in df1.columns:\n",
    "        # Convert the df1 column to datetime, coerce invalids to NaT, then format as mm/dd/yyyy\n",
    "        df1[col] = (\n",
    "            pd.to_datetime(df1[col], errors=\"coerce\")\n",
    "            .dt.strftime(\"%m/%d/%Y\")\n",
    "        )\n",
    "\n",
    "    if col in df2_mapped.columns:\n",
    "        # Convert the df2 column to datetime, coerce invalids to NaT, then format as mm/dd/yyyy\n",
    "        df2_mapped[col] = (\n",
    "            pd.to_datetime(df2_mapped[col], errors=\"coerce\")\n",
    "            .dt.strftime(\"%m/%d/%Y\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combine_dataframes(df1, df2_mapped)\n",
    "df1[\"source\"] = 1\n",
    "df2_mapped[\"source\"] = 2\n",
    "combined_df = pd.concat([df1, df2_mapped], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**: Some quick cleanup, e.g., filling NaNs with empty strings in text columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in combined_df.columns:\n",
    "#     if combined_df[col].dtype == object:\n",
    "#         combined_df[col] = combined_df[col].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Block Data\n",
    "\n",
    "We create candidate pairs only for rows that share the same values in `blocking_keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = block_data(combined_df, blocking_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Fuzzy Comparison Features\n",
    "\n",
    "Using jaro-winkler on the `fuzzy_keys` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = build_comparison_features(combined_df, candidate_pairs, fuzzy_keys)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Classify Duplicates\n",
    "\n",
    "If the average similarity across these fields is >= `similarity_threshold`, \n",
    "they're flagged as duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_idx = classify_duplicates(features_df, similarity_threshold)\n",
    "print(f\"Found {len(duplicates_idx)} pairs classified as duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create De-duped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep the earliest row from each connected group of duplicates.\n",
    "# deduped_df = merge_duplicate_pairs(combined_df, duplicates_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all of Dataset1, add only unmatched Dataset2\n",
    "deduped_df = keep_dataset1_and_unmatched_dataset2(combined_df, duplicates_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deduplicated dataframe has {len(deduped_df)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_df.to_csv(output_path, index=False)\n",
    "print(f\"De-duplicated dataset saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuzzy-dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
